# AI_TASKS.md — Everything Claude Code Builds

> This file is for Claude Code only.
> Work through tasks in strict phase order.
> Do not start a phase until the human confirms their HUMAN_TASKS checklist for that phase is done.
> Before starting any task, read: CLAUDE.md → FRONTEND_CONTEXT.md → this file.

---

## How Claude Code Uses This File

1. Human says "start Phase 1"
2. Claude Code reads CLAUDE.md, FRONTEND_CONTEXT.md, AI_TASKS.md
3. Claude Code works through Phase 1 tasks in order, checking off each one
4. Claude Code stops at the end of Phase 1 and waits
5. Human runs HUMAN_TASKS Phase 1.8 (the Nova Reel generation script)
6. Human confirms clips are generated
7. Then Phase 2 begins

---

## PHASE 1 — Text to ISL (Dictionary + Nova Reel Pre-Generated Clips)

**Human prerequisite:** HUMAN_TASKS 1.1 through 1.7 must be complete before starting.
**Human post-task:** After Task 1.7 below, human runs the generation script (HUMAN_TASKS 1.8).

---

### Task 1.1 — Backend: Add dependencies to requirements.txt

Add to `backend/requirements.txt` if not already present:
```
spacy>=3.7.0
python-dotenv>=1.0.0
boto3>=1.34.0
fastapi>=0.110.0
uvicorn>=0.27.0
python-multipart>=0.0.9
```

After updating, run: `python -m spacy download en_core_web_sm`

- [ ] requirements.txt updated
- [ ] spacy English model downloaded

---

### Task 1.2 — Backend: Create isl_dictionary.json

Create `backend/isl_dictionary.json` with the 50 core words. These must exactly match the filenames that will be generated by the Nova Reel script in Task 1.6:

```json
{
  "hello": "hello.mp4",
  "goodbye": "goodbye.mp4",
  "good": "good.mp4",
  "morning": "morning.mp4",
  "evening": "evening.mp4",
  "night": "night.mp4",
  "how": "how.mp4",
  "are": "are.mp4",
  "you": "you.mp4",
  "i": "i.mp4",
  "me": "me.mp4",
  "my": "my.mp4",
  "name": "name.mp4",
  "is": "is.mp4",
  "what": "what.mp4",
  "where": "where.mp4",
  "when": "when.mp4",
  "yes": "yes.mp4",
  "no": "no.mp4",
  "okay": "okay.mp4",
  "thank": "thank.mp4",
  "please": "please.mp4",
  "sorry": "sorry.mp4",
  "help": "help.mp4",
  "need": "need.mp4",
  "want": "want.mp4",
  "know": "know.mp4",
  "understand": "understand.mp4",
  "come": "come.mp4",
  "go": "go.mp4",
  "stop": "stop.mp4",
  "again": "again.mp4",
  "today": "today.mp4",
  "tomorrow": "tomorrow.mp4",
  "yesterday": "yesterday.mp4",
  "now": "now.mp4",
  "later": "later.mp4",
  "water": "water.mp4",
  "food": "food.mp4",
  "home": "home.mp4",
  "school": "school.mp4",
  "work": "work.mp4",
  "happy": "happy.mp4",
  "sad": "sad.mp4",
  "hot": "hot.mp4",
  "cold": "cold.mp4",
  "big": "big.mp4",
  "small": "small.mp4",
  "new": "new.mp4",
  "UNKNOWN": "unknown.mp4"
}
```

- [ ] isl_dictionary.json created with 50 entries

---

### Task 1.3 — Backend: Create isl_grammar.py

Create `backend/services/isl_grammar.py`:

```python
import spacy
import re

nlp = spacy.load("en_core_web_sm")

# Words to remove — ISL has no articles or linking verbs
DROP_WORDS = {
    "a", "an", "the",
    "is", "am", "are", "was", "were", "be", "been", "being",
    "do", "does", "did",
    "will", "would", "could", "should", "may", "might", "shall",
    "have", "has", "had",
    "it", "its", "this", "that", "these", "those",
    "of", "in", "on", "at", "to", "for", "with", "by", "from",
    "and", "or", "but", "so", "yet",
}

TIME_WORDS = {
    "today", "tomorrow", "yesterday", "now", "later", "soon",
    "morning", "evening", "night", "always", "never", "sometimes",
    "daily", "weekly", "yearly",
}

CONTRACTION_MAP = {
    "don't": "not",
    "doesn't": "not",
    "didn't": "not",
    "can't": "cannot",
    "cannot": "can not",
    "won't": "will not",
    "wouldn't": "would not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "i'm": "i",
    "i've": "i",
    "i'll": "i",
    "i'd": "i",
    "he's": "he",
    "she's": "she",
    "it's": "it",
    "we're": "we",
    "they're": "they",
    "you're": "you",
    "what's": "what",
    "that's": "that",
    "there's": "there",
}


def convert_to_isl_gloss(text: str) -> list[str]:
    """
    Converts English text to ISL gloss token list.
    Rules:
    1. Expand contractions
    2. Remove articles, linking verbs, prepositions, conjunctions
    3. Move time words to front
    4. Uppercase all tokens
    Returns ordered list of ISL gloss tokens.
    """
    # Lowercase and expand contractions
    text = text.lower().strip()
    for contraction, expansion in CONTRACTION_MAP.items():
        text = text.replace(contraction, expansion)

    # Tokenize — remove punctuation
    text = re.sub(r"[^\w\s]", " ", text)
    words = text.split()

    time_tokens = []
    other_tokens = []

    for word in words:
        if not word:
            continue
        if word in DROP_WORDS:
            continue
        if word in TIME_WORDS:
            time_tokens.append(word.upper())
        else:
            other_tokens.append(word.upper())

    return time_tokens + other_tokens
```

- [ ] isl_grammar.py created and tested locally

---

### Task 1.4 — Backend: Create isl_lookup.py

Create `backend/services/isl_lookup.py`:

```python
import json
import os
from pathlib import Path

DICT_PATH = Path(__file__).parent.parent / "isl_dictionary.json"

with open(DICT_PATH) as f:
    DICTIONARY = json.load(f)


def resolve_clips(gloss_tokens: list[str], mode: str = "local") -> list[dict]:
    """
    Maps ISL gloss tokens to video clip URLs.
    mode: 'local' → serves from /clips/ static mount
          's3'    → serves from S3 public bucket
    Returns list of { word, url, found }
    """
    region = os.getenv("AWS_REGION", "ap-south-1")
    bucket = os.getenv("S3_BUCKET_NAME", "samvad-ai-isl-clips")

    results = []
    for token in gloss_tokens:
        key = token.lower()
        filename = DICTIONARY.get(key)
        found = filename is not None
        if not found:
            filename = DICTIONARY.get("UNKNOWN", "unknown.mp4")

        if mode == "s3":
            url = f"https://{bucket}.s3.{region}.amazonaws.com/isl-clips/{filename}"
        else:
            url = f"http://localhost:8000/clips/{filename}"

        results.append({"word": token, "url": url, "found": found})

    return results
```

- [ ] isl_lookup.py created

---

### Task 1.5 — Backend: Create text_to_isl route

Create `backend/routes/text_to_isl.py`:

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from services.isl_grammar import convert_to_isl_gloss
from services.isl_lookup import resolve_clips
import os

router = APIRouter()


class TextToISLRequest(BaseModel):
    text: str
    speed: float = 1.0
    persona: str = "maya"


class ClipItem(BaseModel):
    word: str
    url: str
    found: bool


class TextToISLResponse(BaseModel):
    gloss: list[str]
    clips: list[ClipItem]
    coverage: float
    mode: str


@router.post("/api/text-to-isl", response_model=TextToISLResponse)
def text_to_isl(req: TextToISLRequest):
    if not req.text.strip():
        raise HTTPException(400, "Text cannot be empty")

    mode = os.getenv("ISL_CLIPS_MODE", "local")
    gloss = convert_to_isl_gloss(req.text)

    if not gloss:
        return TextToISLResponse(gloss=[], clips=[], coverage=0.0, mode=mode)

    clips_raw = resolve_clips(gloss, mode)
    found_count = sum(1 for c in clips_raw if c["found"])
    coverage = round(found_count / len(clips_raw), 2) if clips_raw else 0.0

    return TextToISLResponse(
        gloss=gloss,
        clips=[ClipItem(**c) for c in clips_raw],
        coverage=coverage,
        mode=mode,
    )
```

- [ ] text_to_isl.py route created

---

### Task 1.6 — Backend: Create Nova Reel pre-generation script

Create `backend/scripts/generate_isl_clips.py`:

```python
"""
One-time script. Generates ISL video clips for all dictionary words using Amazon Nova Reel.
Run once: python scripts/generate_isl_clips.py
Takes 60-90 minutes. Saves .mp4 files to backend/isl_clips/
"""
import boto3
import json
import os
import time
import sys
from pathlib import Path
from dotenv import load_dotenv

load_dotenv(Path(__file__).parent.parent / ".env")

# Nova Reel only available in us-east-1
bedrock = boto3.client(
    "bedrock-runtime",
    region_name=os.getenv("AWS_BEDROCK_REGION", "us-east-1"),
    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
)

s3 = boto3.client(
    "s3",
    region_name=os.getenv("AWS_BEDROCK_REGION", "us-east-1"),
    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
)

OUTPUT_DIR = Path(__file__).parent.parent / "isl_clips"
OUTPUT_DIR.mkdir(exist_ok=True)

S3_OUTPUT_BUCKET = os.getenv("S3_BUCKET_NAME", "samvad-ai-isl-clips")

WORDS = [
    "hello", "goodbye", "good", "morning", "evening", "night",
    "how", "are", "you", "i", "me", "my", "name", "is",
    "what", "where", "when", "yes", "no", "okay",
    "thank", "please", "sorry", "help", "need", "want",
    "know", "understand", "come", "go", "stop", "again",
    "today", "tomorrow", "yesterday", "now", "later",
    "water", "food", "home", "school", "work",
    "happy", "sad", "hot", "cold", "big", "small", "new",
    "unknown",
]

ISL_PROMPT_TEMPLATE = """A photorealistic video of an Indian Sign Language (ISL) interpreter 
signing the word "{word}". 
Medium close-up shot from waist to head. 
The interpreter is Indian, wearing a solid dark teal top for hand contrast. 
Neutral light grey background. 
Clear, deliberate hand shape and movement for the ISL sign "{word}". 
Natural facial expression appropriate for the sign. 
Professional studio lighting. 
Smooth fluid motion. 24fps."""


def generate_clip(word: str) -> bool:
    output_path = OUTPUT_DIR / f"{word}.mp4"
    if output_path.exists():
        print(f"  ✓ {word}.mp4 already exists, skipping")
        return True

    print(f"  Generating {word}...", end=" ", flush=True)

    prompt = ISL_PROMPT_TEMPLATE.format(word=word.replace("_", " "))
    s3_output_key = f"nova-reel-output/{word}.mp4"

    try:
        response = bedrock.invoke_model(
            modelId="amazon.nova-reel-v1:0",
            contentType="application/json",
            accept="application/json",
            body=json.dumps({
                "taskType": "TEXT_VIDEO",
                "textToVideoParams": {"text": prompt},
                "videoGenerationConfig": {
                    "durationSeconds": 6,
                    "fps": 24,
                    "dimension": "1280x720",
                    "seed": abs(hash(word)) % 2147483647,
                },
                "outputDataConfig": {
                    "s3OutputDataConfig": {
                        "s3Uri": f"s3://{S3_OUTPUT_BUCKET}/{s3_output_key}"
                    }
                },
            }),
        )

        result = json.loads(response["body"].read())
        invocation_arn = result.get("invocationArn")

        if not invocation_arn:
            print(f"FAILED — no invocationArn returned")
            return False

        # Poll for completion
        bedrock_jobs = boto3.client(
            "bedrock",
            region_name=os.getenv("AWS_BEDROCK_REGION", "us-east-1"),
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        )

        for attempt in range(120):  # max 10 min wait
            time.sleep(5)
            job_response = bedrock_jobs.get_async_invoke(
                invocationArn=invocation_arn
            )
            status = job_response["status"]

            if status == "Completed":
                # Download from S3
                s3.download_file(S3_OUTPUT_BUCKET, s3_output_key, str(output_path))
                print(f"done ✓")
                return True
            elif status == "Failed":
                reason = job_response.get("failureMessage", "Unknown")
                print(f"FAILED — {reason}")
                return False
            elif attempt % 6 == 0:
                print(f"waiting...", end=" ", flush=True)

        print(f"TIMEOUT")
        return False

    except Exception as e:
        print(f"ERROR — {e}")
        return False


def main():
    print(f"\nSamvad AI — Nova Reel ISL Clip Generator")
    print(f"Generating {len(WORDS)} clips to {OUTPUT_DIR}")
    print(f"Estimated time: {len(WORDS) * 1.5:.0f} minutes\n")

    success = 0
    failed = []

    for i, word in enumerate(WORDS, 1):
        print(f"[{i}/{len(WORDS)}] {word}")
        if generate_clip(word):
            success += 1
        else:
            failed.append(word)
        time.sleep(2)  # Brief pause between jobs

    print(f"\n{'='*40}")
    print(f"Done. {success}/{len(WORDS)} clips generated.")
    if failed:
        print(f"Failed words: {', '.join(failed)}")
        print("Re-run the script to retry failed words.")
    else:
        print("All clips generated successfully!")
    print(f"Clips saved to: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

- [ ] generate_isl_clips.py created
- [ ] Script reviewed — do NOT run it yet, human must confirm Nova Reel access first

---

### Task 1.7 — Backend: Create S3 upload script for clips

Create `backend/scripts/upload_clips_to_s3.py`:

```python
"""
Run after generate_isl_clips.py to upload local clips to S3.
python scripts/upload_clips_to_s3.py
"""
import boto3, os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv(Path(__file__).parent.parent / ".env")

s3 = boto3.client("s3", region_name=os.getenv("AWS_REGION"))
bucket = os.getenv("S3_BUCKET_NAME")
clips_dir = Path(__file__).parent.parent / "isl_clips"

files = list(clips_dir.glob("*.mp4"))
print(f"Uploading {len(files)} clips to s3://{bucket}/isl-clips/")

for clip in files:
    print(f"  {clip.name}...", end=" ")
    s3.upload_file(
        str(clip), bucket, f"isl-clips/{clip.name}",
        ExtraArgs={"ContentType": "video/mp4"}
    )
    print("✓")

print("Done.")
```

- [ ] upload_clips_to_s3.py created

---

### Task 1.8 — Backend: Update main.py

Add ONLY these lines to `backend/main.py`. Do not remove anything existing:

```python
# Add with other imports at top
from fastapi.staticfiles import StaticFiles
from routes.text_to_isl import router as text_to_isl_router

# Add after app = FastAPI(...) and after CORS middleware
app.mount("/clips", StaticFiles(directory="isl_clips"), name="clips")
app.include_router(text_to_isl_router)
```

Verify CORS middleware allows `http://localhost:5173`. If not present, add:
```python
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "chrome-extension://*"],
    allow_methods=["*"],
    allow_headers=["*"],
)
```

- [ ] main.py updated

---

### Task 1.9 — Frontend: Create islService.ts

Create `samvad-ui/src/services/islService.ts`:

```typescript
export interface ClipItem {
  word: string;
  url: string;
  found: boolean;
}

export interface TextToISLResponse {
  gloss: string[];
  clips: ClipItem[];
  coverage: number;
  mode: string;
}

export async function translateToISL(
  text: string,
  speed: number,
  persona: string
): Promise<TextToISLResponse> {
  const res = await fetch(
    `${import.meta.env.VITE_API_URL}/api/text-to-isl`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text, speed, persona }),
    }
  );
  if (!res.ok) throw new Error(`Translation failed: ${res.status}`);
  return res.json();
}
```

- [ ] islService.ts created

---

### Task 1.10 — Frontend: Create useISLPlayback.ts hook

Create `samvad-ui/src/hooks/useISLPlayback.ts`:

```typescript
import { useState, useRef, useCallback } from 'react';
import { ClipItem } from '../services/islService';

interface PlaybackState {
  isPlaying: boolean;
  currentIndex: number;
  currentWord: string;
}

export function useISLPlayback(
  videoRef: React.RefObject<HTMLVideoElement>
) {
  const [state, setState] = useState<PlaybackState>({
    isPlaying: false,
    currentIndex: -1,
    currentWord: '',
  });

  const playClips = useCallback(
    (clips: ClipItem[], speed: number) => {
      if (!videoRef.current || clips.length === 0) return;

      const video = videoRef.current;

      function loadNext(index: number) {
        if (index >= clips.length) {
          setState({ isPlaying: false, currentIndex: -1, currentWord: '' });
          return;
        }
        const clip = clips[index];
        video.src = clip.url;
        video.playbackRate = speed;
        setState({ isPlaying: true, currentIndex: index, currentWord: clip.word });
        video.play().catch(console.error);
        video.onended = () => loadNext(index + 1);
      }

      loadNext(0);
    },
    []
  );

  const stop = useCallback(() => {
    if (videoRef.current) {
      videoRef.current.pause();
      videoRef.current.src = '';
    }
    setState({ isPlaying: false, currentIndex: -1, currentWord: '' });
  }, []);

  return { ...state, playClips, stop };
}
```

- [ ] useISLPlayback.ts created

---

### Task 1.11 — Frontend: Wire up Live Session Mode tab

Find the Live Session Mode tab component. Make ONLY these changes:

1. Add state:
```typescript
const [inputText, setInputText] = useState('');
const [speed, setSpeed] = useState(1.2);
const [persona, setPersona] = useState('maya');
const [gloss, setGloss] = useState<string[]>([]);
const [isLoading, setIsLoading] = useState(false);
const signerVideoRef = useRef<HTMLVideoElement>(null);
const { isPlaying, currentIndex, currentWord, playClips } = useISLPlayback(signerVideoRef);
```

2. Add handler:
```typescript
const handleTranslate = async () => {
  if (!inputText.trim() || isLoading) return;
  setIsLoading(true);
  try {
    const result = await translateToISL(inputText, speed, persona);
    setGloss(result.gloss);
    playClips(result.clips, speed);
  } catch (e) {
    console.error('Translation error:', e);
  } finally {
    setIsLoading(false);
  }
};
```

3. Wire textarea onChange to `setInputText`
4. Wire speed slider onChange to `setSpeed` (parse float)
5. Wire persona avatar onClick to `setPersona` with 'maya'/'arjun'/'priya'

6. Add translate button directly below textarea:
```tsx
<button
  onClick={handleTranslate}
  disabled={!inputText.trim() || isLoading}
  style={{ opacity: (!inputText.trim() || isLoading) ? 0.5 : 1 }}
  className={/* match existing orange button class in the file */}
>
  {isLoading ? 'Translating...' : 'Translate to ISL'}
</button>
```

7. Add gloss chips below translate button:
```tsx
{gloss.length > 0 && (
  <div style={{ display: 'flex', flexWrap: 'wrap', gap: '6px', marginTop: '8px' }}>
    {gloss.map((token, i) => (
      <span
        key={i}
        style={{
          padding: '2px 10px',
          borderRadius: '999px',
          fontSize: '11px',
          fontFamily: 'monospace',
          background: i === currentIndex ? '#E8531A' : '#F0F0F0',
          color: i === currentIndex ? 'white' : '#555',
          fontWeight: i === currentIndex ? 'bold' : 'normal',
          transition: 'all 0.2s',
        }}
      >
        {token}
      </span>
    ))}
  </div>
)}
```

8. Inside the Signer PiP card — add video element alongside existing avatar image:
```tsx
<video
  ref={signerVideoRef}
  style={{
    display: isPlaying ? 'block' : 'none',
    width: '100%',
    height: '100%',
    objectFit: 'cover',
    borderRadius: 'inherit',
  }}
  playsInline
/>
{/* Keep existing avatar image, hide when playing */}
{/* Add style={{ display: isPlaying ? 'none' : 'block' }} to existing img */}
```

- [ ] Live Session Mode tab wired up
- [ ] Translate button added and functional
- [ ] Gloss chips display correctly
- [ ] Signer PiP card plays video clips

---

### Phase 1 Test Commands

```bash
# Backend
curl http://localhost:8000/api/health
curl -I http://localhost:8000/clips/morning.mp4
curl -X POST http://localhost:8000/api/text-to-isl \
  -H "Content-Type: application/json" \
  -d '{"text": "Good morning how are you", "speed": 1.0, "persona": "maya"}'
```

Expected: gloss = ["MORNING","GOOD","HOW","YOU"], clips array with 4 entries, all found:true

### Phase 1 Complete When:
- [ ] All backend tests pass
- [ ] Typing "Good morning" → clips play in Signer PiP
- [ ] Gloss chips highlight correctly during playback
- [ ] Speed slider changes playback rate
- [ ] Unknown word → UNKNOWN clip plays, no crash

---

## PHASE 2 — Video Upload to ISL PiP

**Human prerequisite:** HUMAN_TASKS 2.1 through 2.4 complete.

---

### Task 2.1 — Backend: Create video_upload.py

Create `backend/services/video_upload.py`:

```python
import boto3
import os
import uuid
from pathlib import Path

def get_s3():
    return boto3.client("s3", region_name=os.getenv("AWS_REGION"))

def upload_video_to_s3(file_bytes: bytes, filename: str) -> dict:
    s3 = get_s3()
    job_id = uuid.uuid4().hex[:10]
    ext = Path(filename).suffix.lower() or ".mp4"
    s3_key = f"uploads/{job_id}{ext}"
    bucket = os.getenv("S3_UPLOAD_BUCKET")

    s3.put_object(
        Bucket=bucket,
        Key=s3_key,
        Body=file_bytes,
        ContentType=f"video/{ext.lstrip('.')}",
    )
    return {
        "job_id": job_id,
        "s3_key": s3_key,
        "s3_uri": f"s3://{bucket}/{s3_key}",
    }
```

- [ ] video_upload.py created

---

### Task 2.2 — Backend: Create transcribe_video.py

Create `backend/services/transcribe_video.py`:

```python
import boto3
import json
import os
import uuid

def get_transcribe():
    return boto3.client("transcribe", region_name=os.getenv("AWS_REGION"))

def get_s3():
    return boto3.client("s3", region_name=os.getenv("AWS_REGION"))

def start_transcription(s3_uri: str, language_code: str = "hi-IN") -> str:
    transcribe = get_transcribe()
    job_name = f"samvad-{uuid.uuid4().hex[:12]}"
    ext = s3_uri.split(".")[-1].lower()
    fmt = ext if ext in ["mp4", "mov", "webm", "mp3", "wav"] else "mp4"

    transcribe.start_transcription_job(
        TranscriptionJobName=job_name,
        Media={"MediaFileUri": s3_uri},
        MediaFormat=fmt,
        LanguageCode=language_code,
        OutputBucketName=os.getenv("S3_UPLOAD_BUCKET"),
        OutputKey=f"transcripts/{job_name}.json",
    )
    return job_name

def get_job_status(job_name: str) -> dict:
    transcribe = get_transcribe()
    resp = transcribe.get_transcription_job(TranscriptionJobName=job_name)
    job = resp["TranscriptionJob"]
    status = job["TranscriptionJobStatus"]

    if status == "IN_PROGRESS":
        return {"status": "processing"}
    if status == "FAILED":
        return {"status": "failed", "reason": job.get("FailureReason", "Unknown")}

    # COMPLETED — fetch output from S3
    s3 = get_s3()
    obj = s3.get_object(
        Bucket=os.getenv("S3_UPLOAD_BUCKET"),
        Key=f"transcripts/{job_name}.json",
    )
    data = json.loads(obj["Body"].read())
    full_text = data["results"]["transcripts"][0]["transcript"]
    items = data["results"]["items"]

    words = []
    for item in items:
        if item["type"] != "pronunciation":
            continue
        words.append({
            "word": item["alternatives"][0]["content"],
            "start_ms": int(float(item["start_time"]) * 1000),
            "end_ms": int(float(item["end_time"]) * 1000),
        })

    return {"status": "complete", "transcript": full_text, "words": words}
```

- [ ] transcribe_video.py created

---

### Task 2.3 — Backend: Create video_to_isl route

Create `backend/routes/video_to_isl.py`:

```python
from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from services.video_upload import upload_video_to_s3
from services.transcribe_video import start_transcription, get_job_status
from services.isl_grammar import convert_to_isl_gloss
from services.isl_lookup import resolve_clips
import os

router = APIRouter()
JOB_CACHE: dict = {}
MAX_BYTES = 500 * 1024 * 1024

ALLOWED_TYPES = {
    "video/mp4", "video/quicktime", "video/webm",
    "video/x-msvideo", "video/mpeg",
}

@router.post("/api/upload-video")
async def upload_video(
    file: UploadFile = File(...),
    language: str = Form(default="hi-IN"),
):
    if file.content_type not in ALLOWED_TYPES:
        raise HTTPException(415, "Unsupported file type. Use MP4, MOV, or WebM.")
    content = await file.read()
    if len(content) > MAX_BYTES:
        raise HTTPException(413, "File too large. Max 500MB.")

    upload = upload_video_to_s3(content, file.filename or "video.mp4")
    job_name = start_transcription(upload["s3_uri"], language)
    JOB_CACHE[upload["job_id"]] = {
        "job_name": job_name,
        "s3_key": upload["s3_key"],
        "language": language,
    }
    return {"job_id": upload["job_id"], "status": "processing"}


@router.get("/api/video-status/{job_id}")
def video_status(job_id: str):
    if job_id not in JOB_CACHE:
        raise HTTPException(404, "Job not found")

    result = get_job_status(JOB_CACHE[job_id]["job_name"])
    if result["status"] != "complete":
        return result

    mode = os.getenv("ISL_CLIPS_MODE", "local")
    clips = []
    for w in result["words"]:
        tokens = convert_to_isl_gloss(w["word"])
        if not tokens:
            continue
        resolved = resolve_clips([tokens[0]], mode)
        c = resolved[0]
        clips.append({**c, "start_ms": w["start_ms"], "end_ms": w["end_ms"]})

    bucket = os.getenv("S3_UPLOAD_BUCKET")
    region = os.getenv("AWS_REGION")
    s3_key = JOB_CACHE[job_id]["s3_key"]
    video_url = f"https://{bucket}.s3.{region}.amazonaws.com/{s3_key}"
    found = sum(1 for c in clips if c["found"])
    coverage = round(found / len(clips), 2) if clips else 0.0

    return {
        "status": "complete",
        "transcript": result["transcript"],
        "clips": clips,
        "video_url": video_url,
        "coverage": coverage,
    }
```

- [ ] video_to_isl.py route created

---

### Task 2.4 — Backend: Register route in main.py

Add:
```python
from routes.video_to_isl import router as video_to_isl_router
app.include_router(video_to_isl_router)
```

- [ ] Route registered

---

### Task 2.5 — Frontend: Create videoService.ts

Create `samvad-ui/src/services/videoService.ts` — full file as defined in PHASE2.md.

- [ ] videoService.ts created

---

### Task 2.6 — Frontend: Create useVideoSync.ts hook

Create `samvad-ui/src/hooks/useVideoSync.ts` — full file as defined in PHASE2.md.

- [ ] useVideoSync.ts created

---

### Task 2.7 — Frontend: Build Streaming tab UI

In the Streaming tab component, build the upload zone, polling logic, and synced video player as specified in PHASE2.md Section 5. Match existing orange/cream color scheme exactly.

- [ ] Upload zone built
- [ ] Polling logic implemented
- [ ] Video player with synced ISL PiP overlay working

---

### Phase 2 Complete When:
- [ ] Upload video → Transcribe job starts → poll returns complete with timestamps
- [ ] Video plays with synced ISL PiP overlay
- [ ] Pause/seek on main video syncs ISL correctly
- [ ] Phase 1 still works

---

## PHASE 3 — Chrome Extension

**Human prerequisite:** HUMAN_TASKS 3.1 through 3.2 complete.

---

### Task 3.1 — Backend: Create transcribe_streaming.py
Full file as defined in PHASE3.md. Requires `amazon-transcribe` package.

- [ ] transcribe_streaming.py created

---

### Task 3.2 — Backend: Create stream_isl.py WebSocket route
Full file as defined in PHASE3.md.

- [ ] stream_isl.py created
- [ ] Route registered in main.py

---

### Task 3.3 — Extension: Create all extension files

Create `samvad-extension/` folder with all files as defined in PHASE3.md:
`manifest.json`, `config.js`, `background.js`, `content.js`, `overlay.css`, `popup.html`, `popup.js`

For icons: create simple 16x16, 48x48, 128x128 orange square PNG files.

- [ ] All extension files created
- [ ] Icons created

---

### Task 3.4 — Extension: Update config.js

Leave `BACKEND_URL` as a placeholder with a clear comment. Human will fill in the ngrok URL.

```javascript
// REPLACE THIS with your ngrok URL each time you restart ngrok
// Run: ngrok http 8000 → copy the https URL → paste below
const BACKEND_URL = 'https://REPLACE_WITH_NGROK_URL.ngrok-free.app';
export { BACKEND_URL };
```

- [ ] config.js created with clear placeholder

---

### Phase 3 Complete When:
- [ ] Extension loads in Chrome without errors
- [ ] YouTube ISL overlay works when enabled
- [ ] Instagram Reels overlay follows scrolling

---

## PHASE 4 — Reverse Mode

**Human prerequisite:** HUMAN_TASKS 4.1 through 4.3 complete (especially signs capture).

---

### Task 4.1 — Backend: Create polly_tts.py
Full file as defined in PHASE4.md.

- [ ] polly_tts.py created

---

### Task 4.2 — Backend: Create capture_sign.py script

Create `backend/scripts/capture_sign.py` — a webcam tool for the human to capture MediaPipe landmarks for each sign. Instructions:
- Opens webcam with OpenCV
- Runs MediaPipe Hands
- Displays live hand skeleton overlay
- On SPACE: captures current landmarks, saves to dict
- On N: moves to next word
- On R: redoes current word
- On Q: saves all to `backend/signs_library.json` and exits

This script is FOR THE HUMAN TO RUN. Not for automated use.

- [ ] capture_sign.py created

---

### Task 4.3 — Backend: Create sign_recognition.py
Full file as defined in PHASE4.md.

- [ ] sign_recognition.py created

---

### Task 4.4 — Backend: Create reverse_mode.py route
Full file as defined in PHASE4.md. Register in main.py.

- [ ] reverse_mode.py created and registered

---

### Task 4.5 — Frontend: Add MediaPipe CDN scripts to index.html

Add before closing `</head>` in `samvad-ui/index.html`:
```html
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/holistic/holistic.js" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
```

- [ ] MediaPipe scripts added to index.html

---

### Task 4.6 — Frontend: Create useSignRecognition.ts hook
Full file as defined in PHASE4.md.

- [ ] useSignRecognition.ts created

---

### Task 4.7 — Frontend: Wire up Reverse Mode toggle

In Live Session Mode tab — wire the existing Reverse Mode toggle to switch between normal mode and webcam signing mode as specified in PHASE4.md.

- [ ] Reverse Mode toggle switches UI correctly
- [ ] Webcam activates (camera only, no microphone)
- [ ] Signs recognized and accumulate into sentence
- [ ] Audio plays after Stop & Generate Voice

---

### Task 4.8 — Frontend: Build Assistive tab

In the Assistive tab (Tab 3) — build the video upload variant of reverse mode as specified in PHASE4.md.

- [ ] Assistive tab video upload flow works end-to-end

---

### Phase 4 Complete When:
- [ ] Reverse Mode toggle works
- [ ] 20 signs recognizable at >80% confidence
- [ ] Polly returns MP3 and it plays + downloads
- [ ] Assistive tab video-to-speech works
- [ ] All previous phases still work